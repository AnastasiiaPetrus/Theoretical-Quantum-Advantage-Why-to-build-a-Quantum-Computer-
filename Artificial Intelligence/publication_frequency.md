# Supplementary Table S2. Standardized and Aggregated Algorithm Frequency (fpub)  
**Domain:** Artificial Intelligence (Keyword: "artificial intelligence AND algorithm")  
**Dataset:** 150 articles (2025) from arXiv, SemanticScholar  

---

## Methodology

- Extracted algorithms and method names from titles, abstracts, keywords, and text snippets.
- Standardized synonymous and closely related variants under unified algorithm names.
- Counted total mentions = 1,165 (aggregate from dataset).
- Normalized frequency \( fpub = \frac{\text{mentions}}{1165} \) to represent relative importance.

---

## Top Algorithms and Methods by Frequency

| Algorithm / Method                                          | Mentions | fpub   |
|------------------------------------------------------------|----------|--------|
| Convolutional Neural Networks (CNN, including ResNet, U-Net, EfficientNet) | 120      | 0.103  |
| Random Forest (RF)                                          | 58       | 0.050  |
| Support Vector Machine (SVM, including variants like Linear SVC) | 55       | 0.047  |
| Gradient Boosting (XGBoost, LightGBM, GB, NGBoost)          | 41       | 0.035  |
| Reinforcement Learning (RL, PPO, Q-Learning, DQN, PbRL)    | 39       | 0.033  |
| Genetic Algorithm (GA) and Metaheuristics (PSO, ACO, WOA, Bee Colony, etc.) | 38       | 0.033  |
| Long Short-Term Memory (LSTM) and Variants (BiLSTM, GRU)   | 37       | 0.032  |
| Decision Tree (DT) and Ensembles (Boosted Trees, Stacked Ensembles) | 31       | 0.027  |
| Transformer Models (including Multi-Head Attention, BERT)  | 30       | 0.026  |
| K-Nearest Neighbors (KNN)                                  | 20       | 0.017  |
| Bayesian Algorithms (Bayesian Optimization, Bayesian Knowledge Transfer) | 17       | 0.015  |
| Autoencoders (VAE, Denoising AE)                           | 15       | 0.013  |
| Graph Neural Networks (GNN, GCN)                           | 15       | 0.013  |
| Kalman Filter and Particle Filter                          | 13       | 0.011  |
| Clustering Algorithms (K-means, Markov Clustering, Fuzzy C-Means) | 13       | 0.011  |
| Na√Øve Bayes Classifier                                     | 11       | 0.009  |
| Formal Verification and Static/Dynamic Analysis Techniques | 10       | 0.009  |
| Monte Carlo Methods (MCMC, Metropolis-Hastings)             | 9        | 0.008  |
| Differential Privacy and Federated Learning (FL)            | 9        | 0.008  |
| YOLO Object Detection Series (v5, v8, v11)                  | 9        | 0.008  |
| Ensemble Learning Methods (Stacking, Bagging, Boosting)     | 8        | 0.007  |
| Adversarial Attack/Defense Methods (PGD, FGSM, etc.)        | 8        | 0.007  |

---

## Selected Extended List (Examples)

| Algorithm / Method                                        | Mentions | fpub   |
|----------------------------------------------------------|----------|--------|
| Backpropagation (BP) Algorithm                           | 7        | 0.006  |
| Thompson Sampling and Bandit Algorithms                  | 6        | 0.005  |
| Fuzzy Logic / Fuzzy Control Algorithms                   | 5        | 0.004  |
| Markov Decision Processes (MDP, POMDP)                   | 5        | 0.004  |
| Particle Swarm Optimization (PSO)                        | 5        | 0.004  |
| Whale Optimization Algorithm (WOA)                       | 5        | 0.004  |
| Siamese Neural Networks                                  | 4        | 0.003  |
| Local Interpretable Model-agnostic Explanations (LIME)  | 4        | 0.003  |
| Shapley Additive Explanations (SHAP)                     | 4        | 0.003  |
| Differential Evolution (DE)                              | 3        | 0.003  |
| Moth Swarm Algorithm (MSA)                              | 3        | 0.003  |
| AlphaDecay Weight Decay Algorithm                        | 3        | 0.003  |
| Chain-of-Thought (CoT) Prompting                         | 3        | 0.003  |
| Natural Gradient Boosting (NGBoost)                      | 3        | 0.003  |
| Metaheuristics (Artificial Bee Colony, Ant Colony, Firefly) | 3        | 0.003  |

---

## Summary and Observations

- **Deep learning** dominates, especially CNN architectures (ResNet, U-Net, EfficientNet), reflecting broad application in vision, NLP, and biomedical fields.
- **Tree-based methods** like Random Forest and Gradient Boosting are widely used for tabular and structured data.
- **Classical ML methods** such as SVM and KNN remain popular for diverse classification tasks.
- **Reinforcement learning** shows strong presence, including PPO, DQN, and preference-based RL variants.
- **Metaheuristic and evolutionary algorithms** (GA, PSO, ACO, WOA, etc.) are heavily used for optimization and parameter tuning.
- **Transformer-based architectures** and attention mechanisms are extensively applied, reflecting LLM and multimodal AI trends.
- **Bayesian and probabilistic methods** contribute in model tuning and uncertainty estimation.
- Emerging focus on **explainable AI** techniques such as SHAP and LIME.
- Specialized algorithms such as **YOLO** family for object detection and **Kalman filters** for estimation appear prominently.
- **Federated learning** and **privacy-preserving** algorithms are gaining traction.
- The data reveal a mix of foundational algorithms and newly evolving methods tailored to specific AI subdomains.

---
